{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPpO1SKydXDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7b076d-0a7d-4193-eb4f-fcb9da90c7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m708.1/708.1 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q openai-whisper gradio google-genai elevenlabs pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g685MjvZdYC0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import whisper\n",
        "import zipfile\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import time\n",
        "import re\n",
        "import time\n",
        "from pydub import AudioSegment\n",
        "from elevenlabs import ElevenLabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koxEbVjhdbGE"
      },
      "outputs": [],
      "source": [
        "class WhisperSTT:\n",
        "    \"\"\"\n",
        "    Speech-to-Text using OpenAI Whisper for Arabic,\n",
        "    with optional Lebanese‚Äêdialect correction via Google Gemini.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_size=\"medium\", gemini_api_key=None):\n",
        "        self.model = whisper.load_model(model_size)\n",
        "        self.model_size = model_size\n",
        "        self.gemini_api_key = gemini_api_key\n",
        "        self.gemini_client = None\n",
        "\n",
        "        if gemini_api_key:\n",
        "            try:\n",
        "                from google import genai\n",
        "                self.gemini_client = genai.Client(api_key=gemini_api_key)\n",
        "                print(\"Gemini text correction enabled\")\n",
        "            except ImportError:\n",
        "                print(\"Warning: google-generativeai package not installed. Using raw transcription.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Failed to initialize Gemini client: {str(e)}. Using raw transcription.\")\n",
        "\n",
        "    def transcribe(self, audio_path):\n",
        "        \"\"\"\n",
        "        Transcribe Arabic speech to text (language=\"ar\").\n",
        "        If Gemini is available, run correction on Lebanese dialect.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(audio_path):\n",
        "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
        "\n",
        "        # 1. Whisper transcription (Arabic)\n",
        "        result = self.model.transcribe(audio_path, language=\"ar\")\n",
        "        transcription = result[\"text\"]\n",
        "\n",
        "        # 2. If Gemini is set up, attempt text correction\n",
        "        if self.gemini_client and self.gemini_api_key:\n",
        "            try:\n",
        "                corrected = self._correct_text(transcription)\n",
        "                if corrected:\n",
        "                    return corrected\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Text correction failed: {str(e)}. Returning raw transcription.\")\n",
        "\n",
        "        return transcription\n",
        "\n",
        "    def _correct_text(self, text):\n",
        "        \"\"\"\n",
        "        Send a prompt to Gemini to correct Lebanese‚Äêdialect text.\n",
        "        Only correct spelling/grammar; no additions.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from google import genai\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            ÿµÿ≠Ÿëÿ≠ ÿßŸÑŸÜÿµ ÿßŸÑÿ™ÿßŸÑŸä ÿßŸÑŸÖŸÉÿ™Ÿàÿ® ÿ®ÿßŸÑŸÑŸáÿ¨ÿ© ÿßŸÑŸÑÿ®ŸÜÿßŸÜŸäÿ©:\n",
        "            - ÿµÿ≠Ÿëÿ≠ ÿßŸÑÿ£ÿÆÿ∑ÿßÿ° ÿßŸÑÿ•ŸÖŸÑÿßÿ¶Ÿäÿ© ŸàÿßŸÑŸÜÿ≠ŸàŸäÿ© ŸÅŸÇÿ∑.\n",
        "            - ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÅŸä ÿ¨ŸÖŸÑÿ© ÿßÿ≥ÿ™ŸÅŸáÿßŸÖŸäÿ©ÿå ÿ∂ŸäŸÅ ÿπŸÑÿßŸÖÿ© ÿßÿ≥ÿ™ŸÅŸáÿßŸÖ.\n",
        "            - ŸÖÿß ÿ™ÿ∂ŸäŸÅ ŸàŸÑÿß ŸÉŸÑŸÖÿ© ÿ≤ŸäÿßÿØÿ© ÿ£Ÿà ÿ¥ÿ±ÿ≠.\n",
        "            - ÿ±ÿ¨Ÿëÿπ ŸÅŸÇÿ∑ ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿµÿ≠ŸéŸëÿ≠ÿå ÿ®ÿØŸàŸÜ ÿπŸÑÿßŸÖÿßÿ™ ÿ™ŸÜÿµŸäÿµ ÿ£Ÿà ÿ£Ÿä ÿ•ÿ∂ÿßŸÅÿßÿ™.\n",
        "            ÿßŸÑŸÜÿµ: \"{text}\"\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.gemini_client.models.generate_content(\n",
        "                model=\"gemini-2.0-flash\", contents=prompt\n",
        "            )\n",
        "            return response.text.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Text correction error: {str(e)}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmiK5IFLeEQs"
      },
      "outputs": [],
      "source": [
        "class MaguyGeminiLLM:\n",
        "    \"\"\"\n",
        "    Enhanced LLM implementation using your proven LangChain + Gemini pattern\n",
        "    Adapted for voice interaction with Maguy Abou Ghosn character\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key, knowledge_base_path=None):\n",
        "        \"\"\"\n",
        "        Initialize with knowledge base integration\n",
        "\n",
        "        Args:\n",
        "            api_key (str): Gemini API key\n",
        "            knowledge_base_path (str): Path to maguy_knowledge_base folder\n",
        "        \"\"\"\n",
        "        # Setup API key (same pattern as your chatbot)\n",
        "        if api_key is None:\n",
        "            try:\n",
        "                from google.colab import userdata\n",
        "                api_key = userdata.get('GEMINI_API_KEY')\n",
        "            except:\n",
        "                api_key = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "        if not api_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY is required!\")\n",
        "\n",
        "        self.client = genai.Client(api_key=api_key)\n",
        "\n",
        "        # Load knowledge base\n",
        "        self.knowledge_base = self._load_knowledge_base(knowledge_base_path)\n",
        "\n",
        "        # Create system instruction with knowledge base\n",
        "        self.system_instruction = self._create_system_instruction()\n",
        "\n",
        "        # Initialize chat\n",
        "        self.chat = self._create_chat()\n",
        "\n",
        "        print(\" MaguyGeminiLLM with Knowledge Base initialized!\")\n",
        "\n",
        "    def _load_knowledge_base(self, kb_path=None):\n",
        "        \"\"\"\n",
        "        Load knowledge base files (adapted from your load_knowledge_base function)\n",
        "        \"\"\"\n",
        "        if kb_path is None:\n",
        "            kb_path = Path(\"maguy_knowledge_base\")\n",
        "        else:\n",
        "            kb_path = Path(kb_path)\n",
        "\n",
        "        if not kb_path.exists():\n",
        "            print(f\"Knowledge base path not found: {kb_path}\")\n",
        "            return \"\"\n",
        "\n",
        "        kb_content = \"\"\n",
        "        for file_path in kb_path.glob('*'):\n",
        "            if file_path.is_file():\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        content = f.read()\n",
        "                        kb_content += f\"\\n\\n### {file_path.stem}\\n{content}\"\n",
        "                        print(f\"Loaded: {file_path.name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file_path.name}: {e}\")\n",
        "        return kb_content\n",
        "\n",
        "    def _load_interview_style(self, interview_path=None):\n",
        "        \"\"\"\n",
        "        Load the annotated interview script for speaking style\n",
        "        \"\"\"\n",
        "        if interview_path is None:\n",
        "            possible_paths = [\n",
        "                \"interview_script.txt\",\n",
        "                \"maguy_interview.txt\",\n",
        "                \"speaking_style.txt\",\n",
        "                \"#ABtalks with Maguy Bou Ghosn - ŸÖÿπ ŸÖÿßÿ∫Ÿä ÿ®Ÿà ÿ∫ÿµŸÜ.txt\"\n",
        "            ]\n",
        "            for path in possible_paths:\n",
        "                if Path(path).exists():\n",
        "                    interview_path = path\n",
        "                    break\n",
        "\n",
        "        if interview_path and Path(interview_path).exists():\n",
        "            try:\n",
        "                with open(interview_path, 'r', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "                    print(f\"Loaded interview style from: {interview_path}\")\n",
        "                    return content\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading interview style: {e}\")\n",
        "\n",
        "        print(\"Interview style file not found\")\n",
        "        return \"\"\n",
        "\n",
        "    def _create_system_instruction(self):\n",
        "        \"\"\"\n",
        "        Create comprehensive system instruction with knowledge base\n",
        "        \"\"\"\n",
        "        base_instruction = \"\"\"\n",
        "        ÿ•ŸÜÿ™Ÿä ŸÖÿßÿ∫Ÿä ÿ®Ÿà ÿ∫ÿµŸÜÿå ÿßŸÑŸÖŸÖÿ´ŸÑÿ© ÿßŸÑŸÑÿ®ŸÜÿßŸÜŸäÿ© ÿßŸÑŸÖÿ¥ŸáŸàÿ±ÿ©. ÿ®ÿ™ÿ≠ŸÉŸä ÿ®ÿßŸÑŸÑŸáÿ¨ÿ© ÿßŸÑŸÑÿ®ŸÜÿßŸÜŸäÿ© ŸÖÿ™ŸÑ ŸÖÿß ÿ•ŸÜÿ™Ÿä ÿ®ÿ™ÿ≠ŸÉŸä ÿ®ÿßŸÑÿ≠Ÿäÿßÿ© ÿßŸÑÿ≠ŸÇŸäŸÇŸäÿ©ÿå ŸÖÿ¥ ÿ®ÿßŸÑŸÅÿµÿ≠Ÿâ ÿ£ÿ®ÿØÿßŸã.\n",
        "        ŸÇŸàÿßÿπÿØ ŸÖŸáŸÖÿ© ŸÑŸÑŸÖÿ≠ÿßÿØÿ´ÿ©:\n",
        "        1. ÿßÿπÿ™ŸÖÿØŸä ÿπŸÑŸâ ÿßŸÑŸÑŸáÿ¨ÿ© ÿßŸÑŸÑÿ®ŸÜÿßŸÜŸäÿ© 100% - ŸÑÿß ÿ™ÿ≥ÿ™ÿπŸÖŸÑŸä ÿßŸÑŸÅÿµÿ≠Ÿâ ÿ£ÿ®ÿØÿßŸã\n",
        "        2. ÿÆŸÑŸä ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉ ŸÇÿµŸäÿ±ÿ© ŸàŸÖŸÜÿßÿ≥ÿ®ÿ© ŸÑŸÑŸÖŸÉÿßŸÑŸÖÿ© ÿßŸÑÿµŸàÿ™Ÿäÿ© (ÿ¨ŸÖŸÑÿ™ŸäŸÜ ŸÑÿ™ŸÑÿ™ ÿ¨ŸÖŸÑ ÿ®ÿßŸÑŸÉÿ™Ÿäÿ±)\n",
        "        3. ŸÉŸàŸÜŸä ÿ∑ÿ®ŸäÿπŸäÿ© ŸàŸàÿØŸàÿØÿ© ŸÖÿ™ŸÑ ÿ¥ÿÆÿµŸäÿ™ŸÉ ÿßŸÑÿ≠ŸÇŸäŸÇŸäÿ©\n",
        "        4. ÿ•ÿ∞ÿß ÿ≥ÿ£ŸÑŸàŸÉ ÿπŸÜ ÿ¥Ÿä ŸÖÿß ÿ®ÿ™ÿπÿ±ŸÅŸäŸáÿå ŸÇŸàŸÑŸä \"ŸÖÿß ÿ®ÿπÿ±ŸÅ\" ÿ®ÿµÿ±ÿßÿ≠ÿ©\n",
        "        5. ÿßÿ≥ÿ™ÿπŸÖŸÑŸä ÿßŸÑÿ™ÿπÿßÿ®Ÿäÿ± ÿßŸÑŸÑÿ®ŸÜÿßŸÜŸäÿ© ÿßŸÑÿ£ÿµŸäŸÑÿ© ŸàÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ≠ŸÉŸäÿ©\n",
        "        6. ÿßŸÑŸÖÿÆÿßÿ∑ÿ® ŸáŸà ÿ¥ÿßÿ®ÿå ŸÖÿ¥ ÿ®ŸÜÿ™ÿå ŸäÿπŸÜŸä ŸÇŸàŸÑŸä \"ŸÉŸäŸÅŸÉÿü\" ŸÖÿ¥ \"ŸÉŸäŸÅŸêŸÉÿü\"ÿå \"ÿßÿ¥ÿ™ŸÇÿ™ŸÑŸÉ\" ŸÖÿ¥ \"ÿßÿ¥ÿ™ŸÇÿ™ŸÑŸêŸÉ\"ÿå ŸàŸáŸÉÿ∞ÿß.\n",
        "\n",
        "        ÿ£ŸÖÿ´ŸÑÿ© ÿπŸÑŸâ ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉ:\n",
        "        - ÿ®ÿØŸÑ \"ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü\" ‚Üí \"ŸÉŸäŸÅŸÉÿü ÿ¥Ÿà ÿ£ÿÆÿ®ÿßÿ±ŸÉÿü\"\n",
        "        - ÿ®ÿØŸÑ \"ÿ£ŸÜÿß ÿ®ÿÆŸäÿ±\" ‚Üí \"ŸÖŸÜŸäÿ≠ÿ© ÿßŸÑÿ≠ŸÖÿØ ŸÑŸÑŸá\"\n",
        "        - ÿ®ÿØŸÑ \"ÿ¥ŸÉÿ±ÿßŸã ŸÑŸÉ\" ‚Üí \"Ÿäÿ≥ŸÑŸÖŸàÿß ÿ•ŸäÿØŸäŸÉ\"\n",
        "        - ÿ®ÿØŸÑ \"ŸÑÿß ÿ£ÿπÿ±ŸÅ\" ‚Üí \"ŸÖÿß ÿ®ÿπÿ±ŸÅ ŸàÿßŸÑŸÑŸá\\\"\"\"\"\n",
        "\n",
        "        if self.knowledge_base:\n",
        "            base_instruction += f\"\\n\\nŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖŸÅÿµŸÑÿ© ÿπŸÜŸÉ:\\n{self.knowledge_base}\"\n",
        "        return base_instruction\n",
        "\n",
        "    def _create_chat(self):\n",
        "        return self.client.chats.create(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            config=types.GenerateContentConfig(\n",
        "                system_instruction=self.system_instruction,\n",
        "                temperature=0.7,\n",
        "                max_output_tokens=120,\n",
        "                top_p=0.9,\n",
        "                top_k=40\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def generate_response(self, user_input, temperature=0.7, max_tokens=120):\n",
        "        \"\"\"\n",
        "        Generate response (same interface as before)\n",
        "        \"\"\"\n",
        "        if not user_input or not user_input.strip():\n",
        "            return \"ÿ¥Ÿà ÿ®ÿØŸÉ ÿ™ÿ≥ÿ£ŸÑŸÜŸäÿü\"\n",
        "\n",
        "        try:\n",
        "            response = self.chat.send_message(user_input)\n",
        "            response_text = response.text.strip()\n",
        "            return self._post_process_response(response_text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response: {str(e)}\")\n",
        "            return \"ŸÖÿπŸÑŸäÿ¥ÿå ÿµÿßÿ± ÿπŸÜÿØŸä ŸÖÿ¥ŸÉŸÑÿ© ÿ™ŸÇŸÜŸäÿ©. ÿ¨ÿ±ÿ® ŸÖÿ±ÿ© ÿ™ÿßŸÜŸäÿ©.\"\n",
        "\n",
        "    def _post_process_response(self, response):\n",
        "        \"\"\"\n",
        "        Enhanced post-processing for Lebanese dialect\n",
        "        \"\"\"\n",
        "        replacements = {\n",
        "            'ÿ£ŸÜÿß': 'ÿßŸÜÿß',\n",
        "            'ÿ£ŸÜÿ™': 'ÿ•ŸÜÿ™',\n",
        "            'ÿ£ŸÜÿ™Ÿê': 'ÿ•ŸÜÿ™Ÿä',\n",
        "            'Ÿáÿ∞ÿß': 'ŸáŸäÿØÿß',\n",
        "            'Ÿáÿ∞Ÿá': 'ŸáŸäÿØŸä',\n",
        "            'ÿ∞ŸÑŸÉ': 'ŸáŸäÿØÿßŸÉ',\n",
        "            'ÿ™ŸÑŸÉ': 'ŸáŸäÿØŸäŸÉ',\n",
        "            'ŸÜÿπŸÖ': 'ÿ¢Ÿá',\n",
        "            'ŸÑÿß': 'ŸÑÿ£',\n",
        "            'ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉ': 'ŸÉŸäŸÅŸÉ',\n",
        "            'ÿ®ÿÆŸäÿ±': 'ŸÖŸÜŸäÿ≠',\n",
        "            'ÿ¥ŸÉÿ±ÿßŸã': 'Ÿäÿ≥ŸÑŸÖŸàÿß',\n",
        "            'ŸÖŸÜ ŸÅÿ∂ŸÑŸÉ': 'ŸÑŸà ÿ≥ŸÖÿ≠ÿ™',\n",
        "            'ŸÖÿπÿ∞ÿ±ÿ©': 'ŸÖÿπŸÑŸäÿ¥',\n",
        "            'ÿ∑ÿ®ÿπÿßŸã': 'ÿ£ŸÉŸäÿØ',\n",
        "            'ŸÖŸÖÿ™ÿßÿ≤': 'ÿ±ÿßÿ¶ÿπ',\n",
        "            'ÿ¨ŸÖŸäŸÑ': 'ÿ≠ŸÑŸà',\n",
        "            'ŸÉÿ´Ÿäÿ±ÿßŸã': 'ŸÉÿ™Ÿäÿ±',\n",
        "            'ŸÇŸÑŸäŸÑÿßŸã': 'ÿ¥ŸàŸä',\n",
        "            'ÿ£ÿ±ŸäÿØ': 'ÿ®ÿØŸä',\n",
        "            'ÿ£ÿ≠ÿ®': 'ÿ®ÿ≠ÿ®'\n",
        "        }\n",
        "        for msa, leb in replacements.items():\n",
        "            response = response.replace(msa, leb)\n",
        "        # 3. Normalize all hamza-alef variants to plain alef\n",
        "        response = re.sub(r'[ÿ•ÿ£ÿ¢]', 'ÿß', response)\n",
        "\n",
        "        return response\n",
        "\n",
        "    def reset_conversation(self):\n",
        "        \"\"\"Reset conversation\"\"\"\n",
        "        self.chat = self._create_chat()\n",
        "        return \"ŸÖÿ±ÿ≠ÿ®ÿß! ÿßŸÜÿß ŸÖÿßÿ∫Ÿä ÿ®Ÿà ÿ∫ÿµŸÜÿå ÿ¥Ÿà ÿ®ÿØŸÉ ÿ™ÿ≠ŸÉŸäŸÑŸäÿü\"\n",
        "\n",
        "    def get_knowledge_summary(self):\n",
        "        \"\"\"Debug function to see what knowledge was loaded\"\"\"\n",
        "        return {\n",
        "            \"knowledge_base_size\": len(self.knowledge_base),\n",
        "            \"interview_style_size\": len(self.interview_style),\n",
        "            \"has_knowledge\": bool(self.knowledge_base),\n",
        "            \"has_interview\": bool(self.interview_style)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NplX9EkReJe5"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('maguy_knowledge_base.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('maguy_knowledge_base')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ElevenLabsTTS:\n",
        "    \"\"\"\n",
        "    High-fidelity ElevenLabs TTS wrapper, using the same settings\n",
        "    as the web UI for your custom Maguy voice.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        api_key: str,\n",
        "        voice_id: str,\n",
        "        output_dir: str = \"outputs\",\n",
        "    ):\n",
        "        if not api_key:\n",
        "            raise ValueError(\"ELEVENLABS_API_KEY is required\")\n",
        "        self.api_key = api_key\n",
        "        self.voice_id = voice_id\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        # Non-streaming endpoint for full quality\n",
        "        self.url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\n",
        "\n",
        "        # These defaults are pulled from your voice definition:\n",
        "        self.settings = {\n",
        "            \"stability\": 1.0,\n",
        "            \"similarity_boost\": 1.0,\n",
        "            \"use_speaker_boost\": True,\n",
        "            \"style\": 0.0,\n",
        "            \"speed\": 0.90\n",
        "        }\n",
        "\n",
        "    def synthesize(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Send text to ElevenLabs, saving an MP3 with web-UI‚Äìmatched settings.\n",
        "        \"\"\"\n",
        "        headers = {\n",
        "            \"xi-api-key\": self.api_key,\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "        payload = {\n",
        "            \"text\": text,\n",
        "            # model_id can remain default; web UI uses its own choice\n",
        "            \"voice_settings\": self.settings\n",
        "        }\n",
        "\n",
        "        resp = requests.post(self.url, json=payload, headers=headers, stream=True)\n",
        "        resp.raise_for_status()\n",
        "\n",
        "        filename = f\"tts_{int(time.time() * 1000)}.mp3\"\n",
        "        path = os.path.join(self.output_dir, filename)\n",
        "        with open(path, \"wb\") as f:\n",
        "            for chunk in resp.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        return path"
      ],
      "metadata": {
        "id": "F5OG5hRnw0GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgeby7iQeKqw",
        "outputId": "b5378335-63e1-4c94-8989-3b3933c72fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42G/1.42G [00:12<00:00, 118MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini text correction enabled\n",
            " MaguyGeminiLLM with Knowledge Base initialized!\n"
          ]
        }
      ],
      "source": [
        "# API keys (or set via ENV / Colab userdata)\n",
        "GEMINI_API_KEY      = \"AIzaSyD1ZVmk2T_cOIZbkD7cJ5OwZEDieB_4_-g\"\n",
        "ELEVENLABS_API_KEY  = \"sk_4b6771baede8fc135e2cffb1f465eb633152b35fed0b29bd\"\n",
        "ELEVENLABS_VOICE_ID = \"vzCewNMrmxS35lNSDJ5T\"\n",
        "\n",
        "# Create Whisper STT (with Gemini correction)\n",
        "stt = WhisperSTT(model_size=\"medium\", gemini_api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Create Maguy Gemini LLM\n",
        "llm = MaguyGeminiLLM(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    knowledge_base_path=\"maguy_knowledge_base\"\n",
        "    )\n",
        "\n",
        "tts = ElevenLabsTTS(api_key=ELEVENLABS_API_KEY, voice_id=ELEVENLABS_VOICE_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfTFaoqeeWtb"
      },
      "outputs": [],
      "source": [
        "def audio_chat(audio_path, chat_history):\n",
        "    \"\"\"\n",
        "    1. Transcribe user audio ‚Üí text (Whisper+Gemini).\n",
        "    2. Generate Maguy‚Äôs text reply via Gemini LLM.\n",
        "    3. Synthesize reply via ElevenLabs REST API.\n",
        "    4. Append messages in OpenAI-chat format to history.\n",
        "    Returns: (history_messages, maguy_audio_path)\n",
        "    \"\"\"\n",
        "    print(\"‚ñ∂Ô∏è audio_chat called\")\n",
        "    print(f\"    audio_path = {audio_path!r}\")\n",
        "\n",
        "    chat_history = chat_history or []\n",
        "\n",
        "    if audio_path is None:\n",
        "        print(\"    No audio received.\")\n",
        "        return chat_history, None\n",
        "\n",
        "    try:\n",
        "        # 1. Transcription\n",
        "        start = time.time()\n",
        "        user_text = stt.transcribe(audio_path)\n",
        "        end = time.time()\n",
        "        print(f\"    Transcription took {end-start:.1f}s, result={user_text!r}\")\n",
        "\n",
        "        # 2. Generate Maguy reply (text)\n",
        "        maguy_text = llm.generate_response(user_text)\n",
        "\n",
        "        # 3. Synthesize via ElevenLabs\n",
        "        maguy_audio_path = tts.synthesize(maguy_text)\n",
        "\n",
        "        # 4. Append to history in OpenAI format\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_text})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": maguy_text})\n",
        "\n",
        "        return chat_history, maguy_audio_path\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"ŸÖÿπŸÑŸäÿ¥ÿå ÿµÿßÿ± ÿπŸÜÿØŸä ŸÖÿ¥ŸÉŸÑÿ© ÿ™ŸÇŸÜŸäÿ©: {e}\"\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
        "        return chat_history, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "ojnyFFy1eYtB",
        "outputId": "36b3900f-c4fc-4df5-90c3-0694f4837f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7dc23cbb76d12d8134.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7dc23cbb76d12d8134.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ñ∂Ô∏è audio_chat called\n",
            "    audio_path = '/tmp/gradio/67eef486da6c508c8f6bf427e21e542a022548552d465cc744a2405028d49e17/audio.wav'\n",
            "    Transcription took 5.1s, result='ÿµÿ®ÿßÿ≠Ÿà ŸÖÿßÿ¨Ÿäÿå ŸÉŸäŸÅŸÉ ÿßŸÑŸäŸàŸÖÿü'\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7dc23cbb76d12d8134.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "with gr.Blocks(\n",
        "    title=\"Interactive Voice Call with Maguy Abou Ghosn\",\n",
        "    css=\"\"\"\n",
        "    /* 1. Import both Tajawal and Montserrat */\n",
        "    @import url('https://fonts.googleapis.com/css2?family=Tajawal:wght@400;600&family=Montserrat:wght@500;700&display=swap');\n",
        "\n",
        "    /* 2. Define your gold & black palette */\n",
        "    :root {\n",
        "      --gold:  #D4AF37; /* classic metallic gold */\n",
        "      --black: #000000; /* deep black */\n",
        "      --white: #FFFFFF; /* for contrast */\n",
        "      --text:  #FFFFFF; /* white text for readability */\n",
        "    }\n",
        "\n",
        "    /* 3. Global body & text */\n",
        "    body {\n",
        "      background-color: var(--black);\n",
        "      color: var(--white);\n",
        "    }\n",
        "\n",
        "    /* 4. Remove default margins/padding on Gradio HTML wrappers */\n",
        "    div[class*=\"html\"] {\n",
        "      margin: 0 !important;\n",
        "      padding: 0 !important;\n",
        "    }\n",
        "\n",
        "    /* 5. Title & subtitle styling */\n",
        "    #title {\n",
        "      font-family: 'Montserrat', sans-serif !important;\n",
        "      font-weight: 700;\n",
        "      font-size: 2rem;\n",
        "      color: var(--gold);\n",
        "      margin: 0 !important;\n",
        "      padding: 0 !important;\n",
        "      text-align: center;\n",
        "    }\n",
        "    #subtitle {\n",
        "      font-family: 'Montserrat', sans-serif !important;\n",
        "      font-weight: 500;\n",
        "      font-size: 1rem;\n",
        "      color: var(--gold);\n",
        "      margin: 0 !important;\n",
        "      padding: 0 0 1em 0 !important;\n",
        "      text-align: center;\n",
        "    }\n",
        "\n",
        "    /* 6. Panels with subtle gold tint */\n",
        "    #left_col, #right_col {\n",
        "      background-color: rgba(212, 175, 55, 0.05);\n",
        "      border: 1px solid var(--gold);\n",
        "      border-radius: 12px;\n",
        "      padding: 15px;\n",
        "      box-shadow: 1px 1px 5px rgba(0,0,0,0.3);\n",
        "      margin-bottom: 0;\n",
        "    }\n",
        "\n",
        "    /* 7. Buttons in gold */\n",
        "    .gradio-container .gr-button {\n",
        "      background-color: var(--gold) !important;\n",
        "      color: var(--black) !important;\n",
        "      border-radius: 8px !important;\n",
        "      transition: filter .1s ease-in-out;\n",
        "    }\n",
        "    .gradio-container .gr-button:hover {\n",
        "      filter: brightness(1.1);\n",
        "    }\n",
        "\n",
        "    /* 8. Footer styling */\n",
        "    #footer {\n",
        "      text-align: center;\n",
        "      color: var(--gold);\n",
        "      font-family: 'Montserrat', sans-serif;\n",
        "      font-size: 1.25rem;\n",
        "      margin-top: 0;\n",
        "      padding-top: 5px;\n",
        "      border-top: 1px solid rgba(255,255,255,0.2);\n",
        "    }\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "\n",
        "    # Header\n",
        "    gr.HTML(\"<h1 id='title'>üé§ Maguy Bou Ghosn Voice Chat</h1>\")\n",
        "    gr.HTML(\"<p id='subtitle'>Speak in Lebanese Arabic and hear Maguy reply like in a real phone call</p>\")\n",
        "\n",
        "    with gr.Group():\n",
        "        with gr.Row():\n",
        "            # Left side: voice interaction\n",
        "            with gr.Column(scale=1, elem_id=\"left_col\"):\n",
        "                audio_in = gr.Audio(\n",
        "                    label=\"üéôÔ∏è Record Your Voice\",\n",
        "                    sources=[\"microphone\", \"upload\"],\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "                submit_btn = gr.Button(\"Send to Maguy\")\n",
        "                audio_out = gr.Audio(\n",
        "                    label=\"üîä Maguy's Voice Reply\",\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "\n",
        "            # Right side: chat history\n",
        "            with gr.Column(scale=2, elem_id=\"right_col\"):\n",
        "                chatbot = gr.Chatbot(\n",
        "                    label=\"üí¨ Chat History\",\n",
        "                    height=443,\n",
        "                    type=\"messages\"\n",
        "                )\n",
        "\n",
        "    # Footer\n",
        "    gr.HTML(\n",
        "        \"<div id='footer'>\"\n",
        "        \"Mohamad Ali Oussayli &nbsp;&amp;&nbsp; Maryam Saghir &nbsp;|&nbsp; Lebanese University\"\n",
        "        \"</div>\"\n",
        "    )\n",
        "\n",
        "    # Click = audio + response flow\n",
        "    submit_btn.click(\n",
        "        fn=audio_chat,\n",
        "        inputs=[audio_in, chatbot],\n",
        "        outputs=[chatbot, audio_out]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}